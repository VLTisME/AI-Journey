{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "TEST_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y = True, as_frame = False)\n",
    "print(X.shape, y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = TEST_SIZE)\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state = 42)\n",
    "X_test, y_test = shuffle(X_test, y_test, random_state = 42)\n",
    "print(X_train.shape, y_train.shape)\n",
    "d = 784\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_params(): This is trash initialization\n",
    "#     W1 = np.random.rand(d, 16) - 0.5\n",
    "#     W2 = np.random.rand(16, 16) - 0.5\n",
    "#     W3 = np.random.rand(16, 10) - 0.5\n",
    "#     B1 = np.random.rand(1, 16) - 0.5\n",
    "#     B2 = np.random.rand(1, 16) - 0.5\n",
    "#     B3 = np.random.rand(1, 10) - 0.5\n",
    "#     return W1, B1, W2, B2, W3, B3\n",
    "\n",
    "def init_params():  # He initialization (take randn then * 2 / dim_of_one_data ;; 0 for all for biases)\n",
    "    W1 = np.random.randn(784, 16) * np.sqrt(2.0 / 784) \n",
    "    B1 = np.zeros((1, 16))  # Bias init as zero  \n",
    "    W2 = np.random.randn(16, 16) * np.sqrt(2.0 / 16)  \n",
    "    B2 = np.zeros((1, 16))  # Bias init as zero  \n",
    "    W3 = np.random.randn(16, 10) * np.sqrt(2.0 / 16)  \n",
    "    B3 = np.zeros((1, 10))  # Bias init as zero  \n",
    "    return W1, B1, W2, B2, W3, B3\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def ReLU_derivative(Z):\n",
    "    return np.where(Z > 0, 1, 0)\n",
    "\n",
    "def stable_softmax(Z):\n",
    "    e_Z = np.exp(Z - np.max(Z, axis = 1, keepdims = True)) # No keepdims = True causes np.max returning (N, ), keepdims -> (N, 1)\n",
    "    return e_Z / np.sum(e_Z, axis = 1, keepdims = True)\n",
    "\n",
    "def forward_propagation(W1, B1, W2, B2, W3, B3, X):\n",
    "    A0 = X\n",
    "    Z1 = A0.dot(W1) + B1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = A1.dot(W2) + B2\n",
    "    A2 = ReLU(Z2)\n",
    "    Z3 = A2.dot(W3) + B3\n",
    "    A3 = stable_softmax(Z3)\n",
    "    return A1, A2, A3, Z1, Z2, Z3\n",
    "\n",
    "def one_hot(Y):\n",
    "    # Y (batch_size, )\n",
    "    # one_hot(Y): (batch_size, 10)\n",
    "    one_hot = np.zeros((Y.size, 10))\n",
    "    one_hot[np.arange(Y.size), Y] = 1\n",
    "    return one_hot\n",
    "\n",
    "def back_propagation(A1, A2, A3, Z1, Z2, Z3, W1, W2, W3, X, Y):\n",
    "    tN = X.shape[0] # in case the last batch is insufficient\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1 / tN * A2.T.dot(dZ3)\n",
    "    dB3 = np.mean(dZ3, axis = 0, keepdims = True)\n",
    "    dA2 = dZ3.dot(W3.T)\n",
    "    dZ2 = dA2 * ReLU_derivative(Z2) # Hadamard product (N x 16) x (N x 16) = (N x 16)\n",
    "    dW2 = 1 / tN * A1.T.dot(dZ2)\n",
    "    dB2 = np.mean(dZ2, axis = 0, keepdims = True)\n",
    "    dA1 = dZ2.dot(W2.T)\n",
    "    dZ1 = dA1 * ReLU_derivative(Z1)\n",
    "    dW1 = 1 / tN * X.T.dot(dZ1)\n",
    "    dB1 = np.mean(dZ1, axis = 0, keepdims = True)\n",
    "    return dB1, dB2, dB3, dW1, dW2, dW3\n",
    "\n",
    "def predict(W1, B1, W2, B2, W3, B3, X):\n",
    "    # X (M x 784)\n",
    "    A0 = X\n",
    "    Z1 = A0.dot(W1) + B1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = A1.dot(W2) + B2\n",
    "    A2 = ReLU(Z2)\n",
    "    Z3 = A2.dot(W3) + B3\n",
    "    A3 = stable_softmax(Z3)\n",
    "    return A3\n",
    "\n",
    "def loss(Y_pred, Y):\n",
    "    # Y (all training imgs, )\n",
    "    # Y_pred (all training imgs, 10)\n",
    "    id0 = np.arange(Y.size)\n",
    "    return -np.mean(np.log(Y_pred[id0, Y])) # np.log output ra 1 vector va lay mean cua vector do va khong keepdims thi ra np 1D?\n",
    "\n",
    "def accuracy(Y_pred, Y):\n",
    "    # Y (all training imgs, )\n",
    "    # Y_pred (all training imgs, 10)\n",
    "    Y_pred = np.argmax(Y_pred, axis = 1)\n",
    "    cnt = np.sum(Y_pred == Y)\n",
    "    return cnt * 100 / Y.size\n",
    "\n",
    "def mini_batch_GD(W1, B1, W2, B2, W3, B3, lr = 0.01, batch_size = 10, num_epoch = 100):\n",
    "    loss_hist = []\n",
    "    acc_hist = []\n",
    "    num_batch = int(X_train.shape[0] / batch_size)\n",
    "    for epoch in range(num_epoch):\n",
    "        mix_id = np.random.permutation(X_train.shape[0])\n",
    "        for i in range(num_batch):\n",
    "            batch_ids = mix_id[batch_size * i : min(X_train.shape[0], batch_size * (i + 1))]\n",
    "            X_batch = X_train[batch_ids]\n",
    "            y_batch = y_train[batch_ids]\n",
    "            one_hot_Y = one_hot(y_batch)\n",
    "            A1, A2, A3, Z1, Z2, Z3 = forward_propagation(W1, B1, W2, B2, W3, B3, X_batch)\n",
    "            dB1, dB2, dB3, dW1, dW2, dW3 = back_propagation(A1, A2, A3, Z1, Z2, Z3, W1, W2, W3, X_batch, one_hot_Y)\n",
    "            W1 = W1 - lr * dW1\n",
    "            W2 = W2 - lr * dW2\n",
    "            W3 = W3 - lr * dW3\n",
    "            B1 = B1 - lr * dB1\n",
    "            B2 = B2 - lr * dB2\n",
    "            B3 = B3 - lr * dB3\n",
    "        Y_pred = predict(W1, B1, W2, B2, W3, B3, X_train)\n",
    "        loss_hist.append(loss(Y_pred, y_train))\n",
    "        acc_hist.append(accuracy(Y_pred, y_train))\n",
    "        print(f\"Epoch: {epoch}; accuracy = {acc_hist[-1]:.2f}%\")\n",
    "    return W1, B1, W2, B2, W3, B3, loss_hist, acc_hist\n",
    "\n",
    "W1, B1, W2, B2, W3, B3 = init_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, B1, W2, B2, W3, B3, loss_hist, acc_hist = mini_batch_GD(W1, B1, W2, B2, W3, B3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = predict(W1, B1, W2, B2, W3, B3, X_test)\n",
    "print(f\"Accuracy on testing data: {accuracy(Y_pred, y_test):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = np.argmax(Y_pred, axis = 1)\n",
    "pred_errors = np.where(Y_hat != y_test)\n",
    "pred_errors = np.array(pred_errors).reshape(-1)\n",
    "pic_errors = X_test[pred_errors]\n",
    "label_errors = y_test[pred_errors]\n",
    "label_pred = Y_hat[pred_errors]\n",
    "fig, ax = plt.subplots(ncols = 5, nrows = 5, figsize = (15, 15))\n",
    "c = 0\n",
    "for i in range(0, 5):\n",
    "    for j in range(0, 5):\n",
    "        ax[i, j].imshow(pic_errors[c].reshape((28, 28)))\n",
    "        ax[i, j].set(title = f\"True label: {label_errors[c]}; pred: {label_pred[c]}\")\n",
    "        ax[i, j].axis('off')\n",
    "        c += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kiến thức:\n",
    "- np.arange(N): tạo mảng np [0, 1,..N]  (có thể thành np.arange(N, d)) với d là bước nhảy\n",
    "- range(N): tương tự như trên nhưng không có bước nhảy và tạo ra range object (not a list)\n",
    "- np.where(Z > 0): trả về index của vị trí có ptu > 0, nếu Z 1D thì ok, 2D thì trả về kiểu ma trận thưa. Nhưng nếu np.where(Z > 0, 1, 0) thì lại trả về nguyên thằng Z, với ptu tại vị trí i, j = 1 nếu Z[i, j] > 0 và ngược lại. Tóm gọn là chỉ ghi đơn giản như np.where(Z > 0) thì y như đang hỏi Z là index nào thỏa mãn? thì trả về những index đó, còn np.where(Z > 0, 1, 0) thì là bảo Z chỗ nào > 0 thì thành 1 và bé hơn 0 thì thành 0 nhé. np.any(Z > 0, axis = 1) != np.any(Z > 0) or np.all(Z > 0).\n",
    "- (dim 1, dim 2, dim 3,...) -> axis 0, axis 1,...\n",
    "- He initialization: Đối với những mạng có ReLU, Bias nên khởi tạo = 0, còn weight thì randn xong nhân với 2 / dims_of_a_data_point"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
