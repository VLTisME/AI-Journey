{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# NBC can encounter underfitting, overfitting like usual. Imagine encounter a new word that has never appeared in training data\n",
    "# -> over/underfit -> NBC can still use cross-validation,... laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction of d5 is ['B']\n",
      "prediction of d6 are [[0.29175335 0.70824665]]\n"
     ]
    }
   ],
   "source": [
    "d1 = [2, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "d2 = [1, 1, 0, 1, 1, 0, 0, 0, 0]\n",
    "d3 = [0, 1, 0, 0, 1, 1, 0, 0, 0]\n",
    "d4 = [0, 1, 0, 0, 0, 0, 1, 1, 1]\n",
    "train_data = np.array([d1, d2, d3, d4])\n",
    "train_label = np.array(['B', 'B', 'B', 'N'])\n",
    "\n",
    "d5 = np.array([[2, 0, 0, 1, 0, 0, 0, 1, 0]])\n",
    "d6 = np.array([[0, 1, 0, 0, 0, 0, 0, 1, 1]])\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(train_data, train_label)\n",
    "\n",
    "y_pred = model.predict(d5)\n",
    "print(f\"prediction of d5 is {model.predict(d5)}\")\n",
    "print(f'prediction of d6 are {model.predict_proba(d6)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-features.txt\n",
      "test-labels.txt\n",
      "train-features-100.txt\n",
      "train-features-400.txt\n",
      "train-features-50.txt\n",
      "train-features.txt\n",
      "train-labels-100.txt\n",
      "train-labels-400.txt\n",
      "train-labels-50.txt\n",
      "train-labels.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "path = os.path.join('data/nbc')\n",
    "for i in os.listdir(path):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2500)\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "<COOrdinate sparse matrix of dtype 'int64'\n",
      "\twith 9599 stored elements and shape (100, 2500)>\n",
      "  Coords\tValues\n",
      "  (0, 563)\t1\n",
      "  (0, 49)\t1\n",
      "  (0, 742)\t1\n",
      "  (0, 1763)\t1\n",
      "  (0, 1867)\t1\n",
      "  (0, 84)\t1\n",
      "  (0, 1815)\t1\n",
      "  (0, 74)\t1\n",
      "  (0, 138)\t1\n",
      "  (0, 776)\t1\n",
      "  (0, 1276)\t1\n",
      "  (0, 199)\t1\n",
      "  (0, 351)\t1\n",
      "  (0, 44)\t1\n",
      "  (0, 18)\t2\n",
      "  (0, 350)\t1\n",
      "  (0, 1130)\t1\n",
      "  (0, 1638)\t1\n",
      "  (0, 512)\t1\n",
      "  (1, 158)\t1\n",
      "  (1, 34)\t5\n",
      "  (1, 96)\t1\n",
      "  (1, 1480)\t1\n",
      "  (1, 976)\t2\n",
      "  (1, 726)\t1\n",
      "  :\t:\n",
      "  (99, 52)\t3\n",
      "  (99, 424)\t2\n",
      "  (99, 33)\t3\n",
      "  (99, 538)\t2\n",
      "  (99, 15)\t1\n",
      "  (99, 258)\t1\n",
      "  (99, 1848)\t3\n",
      "  (99, 73)\t2\n",
      "  (99, 333)\t2\n",
      "  (99, 1281)\t2\n",
      "  (99, 1693)\t2\n",
      "  (99, 2136)\t1\n",
      "  (99, 173)\t1\n",
      "  (99, 228)\t1\n",
      "  (99, 45)\t1\n",
      "  (99, 131)\t1\n",
      "  (99, 17)\t3\n",
      "  (99, 1057)\t1\n",
      "  (99, 171)\t2\n",
      "  (99, 237)\t2\n",
      "  (99, 91)\t1\n",
      "  (99, 53)\t1\n",
      "  (99, 155)\t1\n",
      "  (99, 12)\t1\n",
      "  (99, 104)\t2\n"
     ]
    }
   ],
   "source": [
    "n_words = 2500\n",
    "\n",
    "# train_data = os.path.join(path, 'train-features-100.txt')\n",
    "# with open(train_data) as f:\n",
    "#     content = f.readlines()\n",
    "# print(content)\n",
    "# content = [x.strip() for x in content]\n",
    "# for i in range(len(content)):\n",
    "#     content[i] = content[i].split()\n",
    "# print(content[1])\n",
    "\n",
    "def read_data(data_fn, label_fn):\n",
    "    with open(os.path.join(path, label_fn)) as f:\n",
    "        label = f.readlines()\n",
    "    label = [int(x.strip()) for x in label]\n",
    "    \n",
    "    with open(os.path.join(path, data_fn)) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip().split() for x in content]\n",
    "    dat = np.zeros((len(content), 3), dtype = int)\n",
    "    for i in range(len(content)):\n",
    "        dat[i, :] = np.array([int(content[i][0]), int(content[i][1]), int(content[i][2])])\n",
    "    data = coo_matrix((dat[:, 2], (dat[:, 0] - 1, dat[:, 1] - 1)), shape = (len(label), n_words))\n",
    "    # neu khong ghi gi chi ghi data = coo_matrix(A) thi se tu dong hieu rang A la matrix? va no se sparse nhu binh thuong\n",
    "    # neu ghi nhu tren thi minh se tu modify result cua coo_matrix voi (data, (row, col)) thi cung se lay data la non zeros va index based on row and col arrays\n",
    "    # nhu binh thuong tuc la se co 3 arrays duoc tao: row - col - value (3 arrays do theo cot thi phai)\n",
    "    # len(label) la so luong hang cua ma tran dat no neu nhu no ko co theo style nhu tren, tuc la neu no la 2D array nhu binh thuong, thi hang thu i cua no tuong uong voi document i, va cot thu j cua hang i la so lan xuat hien cua mot tu nao do\n",
    "    # yea va shape = (len(label), n_words) tuc la shape cua dense 2D array\n",
    "    return data, label\n",
    "\n",
    "train_data, train_label = read_data('train-features-100.txt', 'train-labels-100.txt')\n",
    "#print(train_data[:, 1].max()) va hinh nhu la coo_matrix tra ve mot matrix khac size duoc dinh nghia o tren kia\n",
    "print(train_data.shape)\n",
    "print(train_label)\n",
    "print(train_data)\n",
    "# OH WOW, train_data thực sự được lưu dưới dạng sparse matrix nên nó sẽ không lưu các\n",
    "# phần tử = 0, nhưng khi output ra size của nó thì nó vẫn ra size 2D dense matrix :ô\n",
    "# nhưng lưu thì lưu dạng sparse! (có lẽ nó biết size dựa vào việc mình định nghĩa size ở trên)\n",
    "# nếu ko thì nó dựa vào phần tử max của cột và hàng chăng!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 97.69%\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(train_data, train_label)\n",
    "\n",
    "X_test, y_test = read_data('test-features.txt', 'test-labels.txt')\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Accuracy = {accuracy_score(y_pred, y_test) * 100:.2f}%')\n",
    "#print(f\"Predictions for each document are:\\n {model.predict_proba(X_test)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
